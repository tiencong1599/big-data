# YOLOv8 Model Export Guide

This guide shows you how to use `transform_model.py` to export YOLOv8 models for optimal performance in your video processing system.

---

## üìã Prerequisites

### 1. Install Required Packages
```bash
pip install ultralytics torch torchvision
```

### 2. For GPU Support (Recommended)
```bash
# Install PyTorch with CUDA support
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Verify GPU is detected
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
```

### 3. Get YOLOv8 Model
If you don't have `yolov8n.pt`, download it:
```bash
# Download YOLOv8 Nano model (recommended, fastest)
curl -L https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt -o yolov8n.pt
```

---

## üöÄ Quick Start - Export Both Formats (Recommended)

This is the easiest way to get both ONNX and TensorRT models:

```bash
python transform_model.py --input yolov8n.pt --format both
```

**Output:**
- `yolov8n.onnx` - Portable format for CPU/GPU
- `yolov8n.engine` - GPU-optimized TensorRT engine (fastest)

---

## üìñ Usage Examples

### Example 1: Export Only ONNX (CPU/GPU Compatible)
```bash
python transform_model.py --input yolov8n.pt --format onnx
```
- Works on CPU and GPU
- Portable across different hardware
- Requires: `onnxruntime` or `onnxruntime-gpu`

### Example 2: Export Only TensorRT Engine (GPU Only)
```bash
python transform_model.py --input yolov8n.pt --format engine
```
- **Requires NVIDIA GPU**
- 3-10x faster than ONNX
- GPU-specific (not portable to different GPUs)

### Example 3: Export with Custom Image Size
```bash
# Must match YOLO_IMGSZ in spark/config.py (default: 640)
python transform_model.py --input yolov8n.pt --format both --imgsz 640
```

### Example 4: Export with FP32 Precision (More Accurate)
```bash
# Default is FP16 (faster), use --fp16=False for FP32
python transform_model.py --input yolov8n.pt --format engine --imgsz 640
```

### Example 5: Convert Existing ONNX to TensorRT
```bash
# If you already have yolov8n.onnx and want to create .engine
python transform_model.py --onnx-to-engine yolov8n.onnx --output yolov8n.engine
```

---

## üìÅ After Export - Deploy to Project

### Step 1: Copy Model Files
```bash
# Copy exported models to project root
copy yolov8n.onnx F:\BDataFinalProject\yolov8n.onnx
copy yolov8n.engine F:\BDataFinalProject\yolov8n.engine
```

### Step 2: Verify Files
```bash
cd F:\BDataFinalProject
dir yolov8n.*
```

You should see:
```
yolov8n.pt      (Original PyTorch model ~6MB)
yolov8n.onnx    (ONNX model ~6MB)
yolov8n.engine  (TensorRT engine ~12-15MB)
```

### Step 3: Update Configuration (Automatic)

The system automatically detects available models in this priority:
1. `yolov8n.engine` ‚Üí TensorRT GPU (fastest)
2. `yolov8n.onnx` ‚Üí ONNX Runtime (fallback)

**Verify in `spark/config.py`:**
```python
# Auto-detect available model file
if os.path.exists('yolov8n.engine'):
    MODEL_FILE = 'yolov8n.engine'
    print("‚úì Found TensorRT engine model (GPU acceleration)")
elif os.path.exists('yolov8n.onnx'):
    MODEL_FILE = 'yolov8n.onnx'
    print("‚úì Found ONNX model (CPU/GPU)")
```

### Step 4: Update Dockerfile
**Verify `spark/Dockerfile` copies both models:**
```dockerfile
# Copy model files (both ONNX and TensorRT engine)
COPY yolov8n.onnx ./
COPY yolov8n.engine ./
```

### Step 5: Rebuild and Deploy
```bash
# Rebuild Spark container
docker-compose build spark

# Start the service
docker-compose up -d spark

# Check logs to verify model loaded
docker logs spark_processor

# Expected output:
# ‚úì Found TensorRT engine model (GPU acceleration)
# ‚úì TensorRT engine loaded: yolov8n.engine
```

---

## üéØ Model Selection Strategy

### Development (Your Laptop)
- **Use:** `yolov8n.engine` (TensorRT)
- **Why:** Fastest inference on NVIDIA GPU
- **Speed:** 30-50 FPS on 1080p video

### Production Server (With GPU)
- **Use:** `yolov8n.engine` (TensorRT)
- **Why:** Maximum performance
- **Note:** Must rebuild engine on production GPU

### Production Server (CPU Only)
- **Use:** `yolov8n.onnx` (ONNX Runtime)
- **Why:** CPU-compatible fallback
- **Speed:** 8-12 FPS on 1080p video

---

## ‚ö° Performance Comparison

| Model Format | Backend | Device | FPS (1080p) | Latency | Portability |
|-------------|---------|--------|-------------|---------|-------------|
| yolov8n.pt | PyTorch | CPU | 5-8 | 120-200ms | ‚úì High |
| yolov8n.onnx | ONNX RT | CPU | 8-12 | 80-120ms | ‚úì High |
| yolov8n.onnx | ONNX RT | GPU | 15-25 | 40-60ms | ‚úì High |
| **yolov8n.engine** | **TensorRT** | **GPU** | **30-50** | **20-35ms** | ‚úó GPU-specific |

*Tested on NVIDIA RTX 3060*

---

## üîß Troubleshooting

### Error: "CUDA not available"
**Solution:**
```bash
# Check GPU driver
nvidia-smi

# Reinstall PyTorch with CUDA
pip uninstall torch torchvision
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Error: "Model file not found"
**Solution:**
```bash
# Make sure you're in the project root
cd F:\BDataFinalProject

# Check if model exists
dir yolov8n.pt
```

### Error: "TensorRT export failed"
**Solution:**
1. Export ONNX first: `python transform_model.py --input yolov8n.pt --format onnx`
2. Use ONNX in production (still faster than PyTorch)
3. TensorRT is optional for maximum speed

### Warning: "Image size mismatch"
**Solution:**
```bash
# Export with correct size (must match spark/config.py)
python transform_model.py --input yolov8n.pt --format both --imgsz 640
```

The image size in export **must match** `YOLO_IMGSZ` in `spark/config.py`.

---

## üéì Advanced Options

### Different Model Sizes
```bash
# Nano (fastest, recommended)
python transform_model.py --input yolov8n.pt --format both

# Small (more accurate)
python transform_model.py --input yolov8s.pt --format both

# Medium (balanced)
python transform_model.py --input yolov8m.pt --format both
```

### Custom Output Path
```bash
python transform_model.py --input yolov8n.pt --format both --output models/yolov8n.engine
```

### Specific GPU Device
```bash
# If you have multiple GPUs, specify which one
python transform_model.py --input yolov8n.pt --format engine --device 0
```

---

## üìä Complete Workflow

```bash
# 1. Activate virtual environment
cd F:\BDataFinalProject
venv\Scripts\activate

# 2. Export both formats (recommended)
python transform_model.py --input yolov8n.pt --format both --imgsz 640

# 3. Verify exports
dir yolov8n.*

# 4. Rebuild and deploy
docker-compose build spark
docker-compose up -d spark

# 5. Monitor GPU usage
nvidia-smi -l 1

# 6. Check logs
docker logs -f spark_processor

# 7. Access frontend
# Open: http://localhost:4200
```

---

## üìû Support

### Check Model Info
```python
# Quick test script
python -c "from ultralytics import YOLO; model=YOLO('yolov8n.pt'); print(model.info())"
```

### Verify ONNX Model
```bash
pip install onnx
python -c "import onnx; model=onnx.load('yolov8n.onnx'); print(onnx.checker.check_model(model))"
```

### Test Inference Speed
```python
# Create test_inference.py
from ultralytics import YOLO
import time

model = YOLO('yolov8n.engine')  # or yolov8n.onnx
results = model.predict('path/to/test/image.jpg', imgsz=640)
print(f"Inference time: {results[0].speed['inference']:.2f}ms")
```

---

## ‚úÖ Checklist

Before deploying:
- [ ] GPU driver installed and working (`nvidia-smi`)
- [ ] PyTorch with CUDA installed
- [ ] YOLOv8 model downloaded (`yolov8n.pt`)
- [ ] Models exported (`yolov8n.onnx` and/or `yolov8n.engine`)
- [ ] Models copied to project root
- [ ] Image size matches `spark/config.py` (default: 640)
- [ ] Docker containers rebuilt
- [ ] Spark logs show model loaded successfully
- [ ] GPU utilization visible in `nvidia-smi`

---

**Need help?** Check the logs:
```bash
docker logs spark_processor | grep -i "model\|onnx\|tensorrt\|gpu"
```
