from ultralytics import YOLO
import torch
import sys

def check_gpu_status():
    """Kiá»ƒm tra chi tiáº¿t tráº¡ng thÃ¡i GPU vÃ  CUDA"""
    print("=" * 60)
    print("ğŸ” KIá»‚M TRA GPU & CUDA")
    print("=" * 60)
    
    # Kiá»ƒm tra CUDA availability
    cuda_available = torch.cuda.is_available()
    print(f"PyTorch CUDA available: {cuda_available}")
    
    if cuda_available:
        print(f"âœ… GPU Ä‘Æ°á»£c phÃ¡t hiá»‡n: {torch.cuda.get_device_name(0)}")
        print(f"   CUDA Version: {torch.version.cuda}")
        print(f"   GPU Count: {torch.cuda.device_count()}")
        print(f"   Current Device: {torch.cuda.current_device()}")
        print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y GPU hoáº·c CUDA chÆ°a Ä‘Æ°á»£c cÃ i Ä‘Ãºng cÃ¡ch")
        print("\nğŸ“‹ HÆ°á»›ng dáº«n kháº¯c phá»¥c:")
        print("1. Kiá»ƒm tra báº¡n cÃ³ GPU NVIDIA hay khÃ´ng")
        print("2. CÃ i NVIDIA Driver má»›i nháº¥t")
        print("3. CÃ i CUDA Toolkit: https://developer.nvidia.com/cuda-downloads")
        print("4. CÃ i PyTorch vá»›i CUDA support:")
        print("   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
        print("\nâš ï¸ LÆ°u Ã½: Báº¡n váº«n cÃ³ thá»ƒ export ONNX vÃ  cháº¡y trÃªn CPU!")
    
    print("=" * 60)
    return cuda_available

def export_to_tensorrt(model, model_path):
    """Export model sang TensorRT format (.engine)"""
    print("\nğŸš€ EXPORT TO TENSORRT (.engine)")
    print("-" * 60)
    print("â³ Äang export sang TensorRT (cÃ³ thá»ƒ máº¥t 2-5 phÃºt)...")
    
    try:
        model.export(
            format='engine', 
            device=0,           # GPU device
            half=True,          # FP16 precision (faster)
            simplify=True,      # Simplify model
            workspace=4,        # Max workspace size (GB)
            verbose=False
        )
        engine_path = model_path.replace('.pt', '.engine')
        print(f"âœ… TensorRT export SUCCESS: {engine_path}")
        print("   â†’ Sá»­ dá»¥ng Ä‘á»ƒ inference cá»±c nhanh trÃªn GPU")
        return True
    except Exception as e:
        print(f"âŒ TensorRT export FAILED: {e}")
        print("   Kiá»ƒm tra: pip install nvidia-tensorrt")
        return False

def export_to_onnx(model, model_path, use_gpu=False):
    """Export model sang ONNX format"""
    print("\nğŸš€ EXPORT TO ONNX (.onnx)")
    print("-" * 60)
    
    if use_gpu:
        print("â³ Äang export ONNX (GPU-optimized)...")
    else:
        print("â³ Äang export ONNX (CPU-compatible)...")
    
    try:
        model.export(
            format='onnx',
            simplify=True,      # Optimize model graph
            dynamic=False,      # Static shape for better performance
            opset=12,          # ONNX opset version
            verbose=False
        )
        onnx_path = model_path.replace('.pt', '.onnx')
        print(f"âœ… ONNX export SUCCESS: {onnx_path}")
        
        if use_gpu:
            print("   â†’ CÃ³ thá»ƒ cháº¡y trÃªn GPU vá»›i onnxruntime-gpu")
            print("   â†’ CÃ i Ä‘áº·t: pip install onnxruntime-gpu")
        else:
            print("   â†’ CÃ³ thá»ƒ cháº¡y trÃªn CPU vá»›i onnxruntime")
            print("   â†’ CÃ i Ä‘áº·t: pip install onnxruntime")
        
        return True
    except Exception as e:
        print(f"âŒ ONNX export FAILED: {e}")
        return False

def export_model():
    """Main function Ä‘á»ƒ export model"""
    # 1. Kiá»ƒm tra GPU
    has_gpu = check_gpu_status()
    
    # 2. Load model
    model_path = 'F:\\BDataFinalProject\\yolov8n.pt'
    print(f"\nğŸ“¦ Loading model: {model_path}")
    
    try:
        model = YOLO(model_path)
        print("âœ… Model loaded successfully")
    except Exception as e:
        print(f"âŒ KhÃ´ng thá»ƒ load model: {e}")
        return
    
    # 3. Export based on GPU availability
    print("\n" + "=" * 60)
    print("ğŸ”„ Báº®T Äáº¦U EXPORT")
    print("=" * 60)
    
    success_count = 0
    
    if has_gpu:
        # Export to TensorRT (GPU only)
        if export_to_tensorrt(model, model_path):
            success_count += 1
        
        # Export to ONNX (GPU-optimized)
        if export_to_onnx(model, model_path, use_gpu=True):
            success_count += 1
    else:
        # Export to ONNX (CPU-compatible)
        print("\nâš ï¸ KhÃ´ng cÃ³ GPU - chá»‰ export ONNX (CPU mode)")
        if export_to_onnx(model, model_path, use_gpu=False):
            success_count += 1
    
    # 4. Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š Káº¾T QUáº¢")
    print("=" * 60)
    if success_count > 0:
        print(f"âœ… ÄÃ£ export thÃ nh cÃ´ng {success_count} format(s)")
        if has_gpu:
            print("\nğŸ¯ Recommended usage:")
            print("   - TensorRT (.engine): Fastest on GPU")
            print("   - ONNX (.onnx): Portable, works with onnxruntime-gpu")
        else:
            print("\nğŸ¯ ONNX model Ä‘Ã£ sáºµn sÃ ng sá»­ dá»¥ng trÃªn CPU")
    else:
        print("âŒ KhÃ´ng cÃ³ format nÃ o Ä‘Æ°á»£c export thÃ nh cÃ´ng")
    print("=" * 60)

if __name__ == '__main__':
    export_model()