"""
YOLOv8 Model Export Tool
========================
Export YOLOv8 models to ONNX and TensorRT formats for deployment.

USAGE EXAMPLES:
--------------
1. Export both ONNX and TensorRT (recommended):
   python transform_model.py --input yolov8n.pt --both

2. Export only ONNX (CPU/GPU compatible):
   python transform_model.py --input yolov8n.pt --onnx

3. Export only TensorRT engine (GPU only, fastest):
   python transform_model.py --input yolov8n.pt --engine

4. Custom image size:
   python transform_model.py --input yolov8n.pt --both --imgsz 640

5. Convert existing ONNX to TensorRT:
   python transform_model.py --onnx-to-engine yolov8n.onnx

SUPPORTED MODEL SIZES:
---------------------
- yolov8n.pt (Nano) - 3.2M params, fastest, recommended
- yolov8s.pt (Small) - 11.2M params
- yolov8m.pt (Medium) - 25.9M params
- yolov8l.pt (Large) - 43.7M params
- yolov8x.pt (Extra Large) - 68.2M params
"""
import argparse
from ultralytics import YOLO
import torch
import sys
import os

def check_gpu_status():
    """Ki·ªÉm tra chi ti·∫øt tr·∫°ng th√°i GPU v√† CUDA"""
    print("=" * 60)
    print("üîç KI·ªÇM TRA GPU & CUDA")
    print("=" * 60)
    
    # Ki·ªÉm tra CUDA availability
    cuda_available = torch.cuda.is_available()
    print(f"PyTorch CUDA available: {cuda_available}")
    
    if cuda_available:
        print(f"‚úÖ GPU ƒë∆∞·ª£c ph√°t hi·ªán: {torch.cuda.get_device_name(0)}")
        print(f"   CUDA Version: {torch.version.cuda}")
        print(f"   GPU Count: {torch.cuda.device_count()}")
        print(f"   Current Device: {torch.cuda.current_device()}")
        print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y GPU ho·∫∑c CUDA ch∆∞a ƒë∆∞·ª£c c√†i ƒë√∫ng c√°ch")
        print("\nüìã H∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c:")
        print("1. Ki·ªÉm tra b·∫°n c√≥ GPU NVIDIA hay kh√¥ng")
        print("2. C√†i NVIDIA Driver m·ªõi nh·∫•t")
        print("3. C√†i CUDA Toolkit: https://developer.nvidia.com/cuda-downloads")
        print("4. C√†i PyTorch v·ªõi CUDA support:")
        print("   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
        print("\n‚ö†Ô∏è L∆∞u √Ω: B·∫°n v·∫´n c√≥ th·ªÉ export ONNX v√† ch·∫°y tr√™n CPU!")
    
    print("=" * 60)
    return cuda_available

def export_to_tensorrt(model, model_path):
    """Export model sang TensorRT format (.engine)"""
    print("\nüöÄ EXPORT TO TENSORRT (.engine)")
    print("-" * 60)
    print("‚è≥ ƒêang export sang TensorRT (c√≥ th·ªÉ m·∫•t 2-5 ph√∫t)...")
    
    try:
        model.export(
            format='engine', 
            device=0,           # GPU device
            half=True,          # FP16 precision (faster)
            simplify=True,      # Simplify model
            workspace=4,        # Max workspace size (GB)
            verbose=False
        )
        engine_path = model_path.replace('.pt', '.engine')
        print(f"‚úÖ TensorRT export SUCCESS: {engine_path}")
        print("   ‚Üí S·ª≠ d·ª•ng ƒë·ªÉ inference c·ª±c nhanh tr√™n GPU")
        return True
    except Exception as e:
        print(f"‚ùå TensorRT export FAILED: {e}")
        print("   Ki·ªÉm tra: pip install nvidia-tensorrt")
        return False

def export_to_onnx(model, model_path, use_gpu=False):
    """Export model sang ONNX format"""
    print("\nüöÄ EXPORT TO ONNX (.onnx)")
    print("-" * 60)
    
    if use_gpu:
        print("‚è≥ ƒêang export ONNX (GPU-optimized)...")
    else:
        print("‚è≥ ƒêang export ONNX (CPU-compatible)...")
    
    try:
        model.export(
            format='onnx',
            simplify=True,      # Optimize model graph
            dynamic=False,      # Static shape for better performance
            opset=12,          # ONNX opset version
            verbose=False
        )
        onnx_path = model_path.replace('.pt', '.onnx')
        print(f"‚úÖ ONNX export SUCCESS: {onnx_path}")
        
        if use_gpu:
            print("   ‚Üí C√≥ th·ªÉ ch·∫°y tr√™n GPU v·ªõi onnxruntime-gpu")
            print("   ‚Üí C√†i ƒë·∫∑t: pip install onnxruntime-gpu")
        else:
            print("   ‚Üí C√≥ th·ªÉ ch·∫°y tr√™n CPU v·ªõi onnxruntime")
            print("   ‚Üí C√†i ƒë·∫∑t: pip install onnxruntime")
        
        return True
    except Exception as e:
        print(f"‚ùå ONNX export FAILED: {e}")
        return False

def export_model():
    """Main function ƒë·ªÉ export model"""
    # 1. Ki·ªÉm tra GPU
    has_gpu = check_gpu_status()
    
    # 2. Load model
    model_path = 'F:\\BDataFinalProject\\yolov8n.pt'
    print(f"\nüì¶ Loading model: {model_path}")
    
    try:
        model = YOLO(model_path)
        print("‚úÖ Model loaded successfully")
    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ load model: {e}")
        return
    
    # 3. Export based on GPU availability
    print("\n" + "=" * 60)
    print("üîÑ B·∫ÆT ƒê·∫¶U EXPORT")
    print("=" * 60)
    
    success_count = 0
    
    if has_gpu:
        # Export to TensorRT (GPU only)
        if export_to_tensorrt(model, model_path):
            success_count += 1
        
        # Export to ONNX (GPU-optimized)
        if export_to_onnx(model, model_path, use_gpu=True):
            success_count += 1
    else:
        # Export to ONNX (CPU-compatible)
        print("\n‚ö†Ô∏è Kh√¥ng c√≥ GPU - ch·ªâ export ONNX (CPU mode)")
        if export_to_onnx(model, model_path, use_gpu=False):
            success_count += 1
    
    # 4. Summary
    print("\n" + "=" * 60)
    print("üìä K·∫æT QU·∫¢")
    print("=" * 60)
    if success_count > 0:
        print(f"‚úÖ ƒê√£ export th√†nh c√¥ng {success_count} format(s)")
        if has_gpu:
            print("\nüéØ Recommended usage:")
            print("   - TensorRT (.engine): Fastest on GPU")
            print("   - ONNX (.onnx): Portable, works with onnxruntime-gpu")
        else:
            print("\nüéØ ONNX model ƒë√£ s·∫µn s√†ng s·ª≠ d·ª•ng tr√™n CPU")
    else:
        print("‚ùå Kh√¥ng c√≥ format n√†o ƒë∆∞·ª£c export th√†nh c√¥ng")
    print("=" * 60)

def export_to_onnx(model_path, output_path='yolov8n.onnx', imgsz=640):
    """Export PyTorch model to ONNX format"""
    print(f"Loading model: {model_path}")
    model = YOLO(model_path)
    
    print(f"Exporting to ONNX: {output_path}")
    model.export(
        format='onnx',
        imgsz=imgsz,
        simplify=True,
        opset=12,
        dynamic=False
    )
    print(f"‚úì ONNX model saved: {output_path}")

def export_to_tensorrt(model_path, output_path='yolov8n.engine', imgsz=640, fp16=True, device=0):
    """Export PyTorch model to TensorRT engine format"""
    print(f"Loading model: {model_path}")
    model = YOLO(model_path)
    
    print(f"Exporting to TensorRT: {output_path}")
    print(f"  Image size: {imgsz}x{imgsz}")
    print(f"  FP16: {fp16}")
    print(f"  Device: cuda:{device}")
    
    model.export(
        format='engine',
        imgsz=imgsz,
        half=fp16,  # Use FP16 precision for faster inference
        device=device,
        workspace=4,  # Max workspace size in GB
        verbose=True
    )
    print(f"‚úì TensorRT engine saved: {output_path}")
    print("\nNote: TensorRT engines are GPU-specific. This engine is optimized for your current GPU.")

def convert_onnx_to_tensorrt(onnx_path, engine_path='yolov8n.engine', fp16=True, device=0):
    """Convert ONNX model to TensorRT engine using trtexec"""
    import subprocess
    
    print(f"Converting ONNX to TensorRT engine...")
    print(f"  Input: {onnx_path}")
    print(f"  Output: {engine_path}")
    print(f"  FP16: {fp16}")
    
    cmd = [
        'trtexec',
        f'--onnx={onnx_path}',
        f'--saveEngine={engine_path}',
        '--explicitBatch',
    ]
    
    if fp16:
        cmd.append('--fp16')
    
    try:
        subprocess.run(cmd, check=True)
        print(f"‚úì TensorRT engine saved: {engine_path}")
    except FileNotFoundError:
        print("Error: trtexec not found. Install TensorRT: pip install tensorrt")
    except subprocess.CalledProcessError as e:
        print(f"Error during conversion: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Convert YOLOv8 models to different formats')
    parser.add_argument('--input', default='yolov8n.pt', help='Input model path (.pt)')
    parser.add_argument('--output', default='yolov8n.engine', help='Output file path')
    parser.add_argument('--format', choices=['onnx', 'engine', 'both'], default='both',
                       help='Export format: onnx, engine, or both')
    parser.add_argument('--imgsz', type=int, default=640, help='Image size (default: 640)')
    parser.add_argument('--fp16', action='store_true', default=True, help='Use FP16 precision')
    parser.add_argument('--device', type=int, default=0, help='CUDA device id')
    parser.add_argument('--onnx-to-engine', help='Convert existing ONNX to TensorRT engine')
    
    args = parser.parse_args()
    
    if args.onnx_to_engine:
        # Convert existing ONNX to TensorRT
        convert_onnx_to_tensorrt(args.onnx_to_engine, args.output, args.fp16, args.device)
    else:
        # Export from PyTorch
        if args.format in ['onnx', 'both']:
            onnx_output = args.output if args.format == 'onnx' else 'yolov8n.onnx'
            export_to_onnx(args.input, onnx_output, args.imgsz)
        
        if args.format in ['engine', 'both']:
            engine_output = args.output if args.format == 'engine' else 'yolov8n.engine'
            export_to_tensorrt(args.input, engine_output, args.imgsz, args.fp16, args.device)
    
    print("\n=== Model Conversion Complete ===")
    print("\nUsage:")
    print("1. Place yolov8n.engine in project root for GPU acceleration")
    print("2. Place yolov8n.onnx as fallback for CPU/non-TensorRT environments")
    print("3. Rebuild Docker: docker-compose build spark")
    print("4. Deploy: docker-compose up -d spark")